{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from flask import Flask, request, jsonify, render_template\n",
        "import os\n"
      ],
      "metadata": {
        "id": "h97Qo7FtFIDa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/dataset_full'"
      ],
      "metadata": {
        "id": "xAplpO-VAw4M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "def load_images(file_path, target_size=(256, 256)):\n",
        "    images = []\n",
        "    labels = []\n",
        "    label_map = {}\n",
        "    label_counter = 0\n",
        "    for folder in os.listdir(file_path):\n",
        "        label_map[folder] = label_counter\n",
        "        for file in os.listdir(os.path.join(file_path, folder)):\n",
        "            image_path = os.path.join(file_path, folder, file)\n",
        "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read image in grayscale\n",
        "            # Preprocessing steps (resize, normalize, etc.)\n",
        "            if image is not None:\n",
        "                # Resize image to target size\n",
        "                image = cv2.resize(image, target_size)\n",
        "                images.append(image)\n",
        "                labels.append(label_map[folder])\n",
        "        label_counter += 1\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sezgPLQ6FNm9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Images are read using OpenCV's cv2.imread() function with the flag cv2.IMREAD_GRAYSCALE, which reads the image in grayscale mode. Grayscale images have only one channel representing pixel intensity, which simplifies processing.\n",
        "\n",
        "After reading each image, it is resized to a target size specified by the target_size parameter. Resizing is important for ensuring consistency in the dimensions of all images. In this case, the target size is set to (256, 256) pixels"
      ],
      "metadata": {
        "id": "m23wWcnv8kmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(images):\n",
        "    features = []\n",
        "    for image in images:\n",
        "        equalized = cv2.equalizeHist(image)\n",
        "        hist_equalized, _ = np.histogram(equalized.ravel(), bins=256, range=[0, 256])\n",
        "\n",
        "        # 2. Grayscale Transformation\n",
        "        gray_transform = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "        # 3. Canny Edge Detection\n",
        "        edges = cv2.Canny(image, 100, 200)  # Adjust threshold values as needed\n",
        "        edges_hist, _ = np.histogram(edges.ravel(), bins=256, range=[0, 256])\n",
        "\n",
        "        # Append features to the feature vector\n",
        "        features.append(np.concatenate([hist_equalized, edges_hist]))\n",
        "\n",
        "    return np.array(features)\n"
      ],
      "metadata": {
        "id": "vGTuFrr8Fdkr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram Equalization: Enhances contrast and reveals hidden details in the image, making it easier for the classifier to distinguish between different classes based on intensity variations.\n",
        "\n",
        "Grayscale Transformation: Simplifies the representation of the image by reducing it to a single channel, thereby reducing computational complexity while retaining essential information about intensity values.\n",
        "\n",
        "Canny Edge Detection: Identifies significant changes in intensity, which often correspond to edges and boundaries between objects in the image. These edges contain important structural information that can be valuable for classification, helping the model to recognize distinct patterns and shapes."
      ],
      "metadata": {
        "id": "DLNJHCUr97lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for dimensionality reduction (PCA)\n",
        "def apply_pca(features, n_components=100):\n",
        "    pca = PCA(n_components=n_components)\n",
        "    reduced_features = pca.fit_transform(features)\n",
        "    return reduced_features"
      ],
      "metadata": {
        "id": "z58pt8v_FuGO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `apply_pca` function performs Principal Component Analysis (PCA) on the input feature set to reduce its dimensionality. By specifying the number of components, it transforms the original features into a lower-dimensional space while retaining the most important information. This technique is crucial for managing high-dimensional data in image classification tasks, as it helps in mitigating computational complexity, overfitting, and facilitates better visualization and interpretation of the data."
      ],
      "metadata": {
        "id": "7XpBLOtB-2vO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train and evaluate model\n",
        "def train_evaluate_model(X_train, X_test, y_train, y_test):\n",
        "    # Train model\n",
        "    clf = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    # Evaluate model\n",
        "    y_pred_train = clf.predict(X_train)\n",
        "    y_pred_test = clf.predict(X_test)\n",
        "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "    return clf, train_accuracy, test_accuracy"
      ],
      "metadata": {
        "id": "SjcwOpG9Fxfl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/dataset_full\"\n",
        "images, labels = load_images(file_path)"
      ],
      "metadata": {
        "id": "7nwKOJL5F5Cd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = extract_features(images)"
      ],
      "metadata": {
        "id": "0ETxCtbbMjrp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_features = apply_pca(features)"
      ],
      "metadata": {
        "id": "ejNhLW4xNmdr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(reduced_features, labels, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "2vVE7PkWYyE4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_accuracy, test_accuracy = train_evaluate_model(X_train, X_test, y_train, y_test)\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALmu1I9WZCre",
        "outputId": "e047e170-821c-4c4c-af7b-1dd2dd715bdf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 0.2857142857142857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def train_evaluate_model(X_train, X_test, y_train, y_test):\n",
        "    # Train model\n",
        "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    # Evaluate model\n",
        "    y_pred_train = clf.predict(X_train)\n",
        "    y_pred_test = clf.predict(X_test)\n",
        "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "    return clf, train_accuracy, test_accuracy\n"
      ],
      "metadata": {
        "id": "HR4Sim9N3aYe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_accuracy, test_accuracy = train_evaluate_model(X_train, X_test, y_train, y_test)\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqELLwTL3yCI",
        "outputId": "3cda8148-3d42-48cd-fadd-2eae787474c7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 0.2857142857142857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def train_evaluate_model(X_train, X_test, y_train, y_test):\n",
        "    # Train model\n",
        "    clf = KNeighborsClassifier(n_neighbors=5)  # Set the number of neighbors (you can adjust this)\n",
        "    clf.fit(X_train, y_train)\n",
        "    # Evaluate model\n",
        "    y_pred_train = clf.predict(X_train)\n",
        "    y_pred_test = clf.predict(X_test)\n",
        "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "    return clf, train_accuracy, test_accuracy\n"
      ],
      "metadata": {
        "id": "orliUitl31eK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_accuracy, test_accuracy = train_evaluate_model(X_train, X_test, y_train, y_test)\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2txIqu7Z4CO_",
        "outputId": "9a03f75e-038b-4b5d-e127-0b1592ad6c09"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.4965986394557823\n",
            "Test Accuracy: 0.37566137566137564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to constraints posed by a slow machine processor, the dataset size was reduced to facilitate smoother processing, leading to a compromise in model accuracy across all models."
      ],
      "metadata": {
        "id": "QJFV5hsnDuzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve model performance, consider data augmentation to enrich the dataset and hyperparameter optimization for fine-tuning model settings. Additionally, explore transfer learning with pre-trained deep learning models for feature extraction. Ensemble learning methods like model stacking and automated feature selection techniques can further enhance classification accuracy while streamlining the feature extraction process."
      ],
      "metadata": {
        "id": "pPEoY7GdGnX1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xVzq21dGGoq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rUiAKfE4HYWb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}